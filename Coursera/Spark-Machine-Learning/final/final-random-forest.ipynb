{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72d100a6-e6de-44b5-8c99-b0a80b285ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "import os\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark2pmml import PMMLBuilder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import logging\n",
    "import shutil\n",
    "import site\n",
    "import sys\n",
    "import wget\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06de7e33-1dd7-4771-a6c5-cc1680ea6d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parquet = os.environ.get('data_parquet',\n",
    "                              'data.parquet')  # input file name (parquet)\n",
    "master = os.environ.get('master',\n",
    "                        \"local[*]\")  # URL to Spark master\n",
    "model_target = os.environ.get('model_target',\n",
    "                              \"model.xml\")  # model output file name\n",
    "data_dir = os.environ.get('data_dir',\n",
    "                          '../../data/')  # temporary directory for data\n",
    "input_columns = os.environ.get('input_columns',\n",
    "                               '[\"x\", \"y\", \"z\"]')  # input columns to consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b92635f-2538-415e-b4d4-6ad85754fdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = list(\n",
    "    map(lambda s: re.sub('$', '\"', s),\n",
    "        map(\n",
    "            lambda s: s.replace('=', '=\"'),\n",
    "            filter(\n",
    "                lambda s: s.find('=') > -1 and bool(re.match(r'[A-Za-z0-9_]*=[.\\/A-Za-z0-9]*', s)),\n",
    "                sys.argv\n",
    "            )\n",
    "    )))\n",
    "\n",
    "for parameter in parameters:\n",
    "    logging.warning('Parameter: ' + parameter)\n",
    "    exec(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30041165-7185-420e-a97b-2160ccbca378",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/04 03:09:20 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/10/04 03:09:23 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setMaster(master)\n",
    "#if sys.version[0:3] == '3.6' or sys.version[0:3] == '3.7':\n",
    "conf.set(\"spark.jars\", 'jpmml-sparkml-executable-1.5.12.jar')\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = sqlContext.sparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34dfefb9-dfc0-46f5-a3cc-a9791fc29f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(data_dir + data_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7044dd51-b5c3-45bb-aca0-0e25a6036f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "df = df.withColumn(\"x\", df.x.cast(DoubleType()))\n",
    "df = df.withColumn(\"y\", df.y.cast(DoubleType()))\n",
    "df = df.withColumn(\"z\", df.z.cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56cc7b68-831d-485b-beb7-2861324060be",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = df.randomSplit([0.8, 0.2], seed=1)\n",
    "df_train = splits[0]\n",
    "df_test = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e2e6e92-21bd-48ac-8fc0-8f3764dc932f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols=eval(input_columns),\n",
    "                                  outputCol=\"features\")\n",
    "indexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66acad7e-7188-447e-8ebb-507209786ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters: 10 5 accuracy: 0.4413014987566369\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters: 10 7 accuracy: 0.45918464502542733\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters: 20 5 accuracy: 0.4422872280843247\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 91:=================================================>        (6 + 1) / 7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters: 20 7 accuracy: 0.4644745390595246\n",
      "---------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "numTrees_list = [10, 20]\n",
    "maxDepth_list = [5, 7]\n",
    "\n",
    "for numTrees_value in numTrees_list:\n",
    "    for maxDepth_value in maxDepth_list:\n",
    "        search_rf = RandomForestClassifier(numTrees=numTrees_value, maxDepth=maxDepth_value)\n",
    "        pipeline = Pipeline(stages=[indexer,vectorAssembler, search_rf])\n",
    "        model = pipeline.fit(df_train)\n",
    "        prediction = model.transform(df_train)\n",
    "        binEval = MulticlassClassificationEvaluator().setMetricName(\"accuracy\").setPredictionCol(\"prediction\").setLabelCol(\"label\")\n",
    "        print(f'Model Parameters: {numTrees_value} {maxDepth_value} accuracy: {binEval.evaluate(prediction)}')\n",
    "        print('---------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db6d694-0077-48f1-a3f6-a4a71ff2d6ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
